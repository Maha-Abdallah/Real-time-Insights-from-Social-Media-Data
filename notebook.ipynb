{"cells":[{"source":"## 1. Local and global thought patterns\n<p>While we might not be Twitter fans, we have to admit that it has a huge influence on the world (who doesn't know about Trump's tweets). Twitter data is not only gold in terms of insights, but <strong><em>Twitter-storms are available for analysis in near real-time</em></strong>. This means we can learn about the big waves of thoughts and moods around the world as they arise. </p>\n<p>As any place filled with riches, Twitter has <em>security guards</em> blocking us from laying our hands on the data right away ‚õîÔ∏è Some  authentication steps (really straightforward) are needed to call their APIs for data collection. Since our goal today is learning to extract insights from data, we have already gotten a green-pass from security ‚úÖ Our data is ready for usage in the datasets folder ‚Äî we can concentrate on the fun part! üïµÔ∏è‚Äç‚ôÄÔ∏èüåé\n<br>\n<br>\n<img src=\"https://assets.datacamp.com/production/project_760/img/tweets_influence.png\" style=\"width: 300px\">\n<hr>\n<br>Twitter provides both global and local trends. Let's load and inspect data for topics that were hot worldwide (WW) and in the United States (US) at the moment of query  ‚Äî snapshot of JSON response from the call to Twitter's <i>GET trends/place</i> API.</p>\n<p><i><b>Note</b>: <a href=\"https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place.html\">Here</a> is the documentation for this call, and <a href=\"https://developer.twitter.com/en/docs/api-reference-index.html\">here</a> a full overview on Twitter's APIs.</i></p>","metadata":{"dc":{"key":"3"},"run_control":{"frozen":true},"tags":["context"]},"id":"0779e2bd-b99b-451a-a6a2-34aecd5887b6","cell_type":"markdown"},{"source":"# Load the jsonlite library for working with JSON data\ninstall.packages(\"jsonlite\")\nlibrary(jsonlite)\n\n# Loading WW_trends and US_trends data\nWW_trends <- fromJSON(file = 'datasets/WWTrends.json')\nUS_trends <- fromJSON(file = 'datasets/USTrends.json')\n\n# Inspecting data by printing out WW_trends and US_trends variables\nprint(WW_trends)\nprint(US_trends)\n","metadata":{"dc":{"key":"3"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"8974291b-2b75-4edf-9e29-310725093e0c","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 2. Prettifying the output\n<p>Our data was hard to read! Luckily, we can resort to the <i>json.dumps()</i> method to have it formatted as a pretty JSON string.</p>","metadata":{"dc":{"key":"10"},"run_control":{"frozen":true},"tags":["context"]},"id":"49a02466-ab59-4226-bc83-ec1a06fc4886","cell_type":"markdown"},{"source":"# Install and load necessary library\ninstall.packages(\"jsonlite\")\nlibrary(jsonlite)\n\n# Assuming WW_trends and US_trends are lists or data frames containing your trends data\n\n# Pretty-printing the results for WW trends\ncat(\"WW trends:\\n\")\ncat(toJSON(WW_trends, pretty = TRUE), \"\\n\")\n\n# Pretty-printing the results for US trends\ncat(\"\\nUS trends:\\n\")\ncat(toJSON(US_trends, pretty = TRUE), \"\\n\")\n","metadata":{"dc":{"key":"10"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"4fcc1bc4-de65-46e5-b30b-9247dacd779f","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 3.  Finding common trends\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è From the pretty-printed results (output of the previous task), we can observe that:</p>\n<ul>\n<li><p>We have an array of trend objects having: the name of the trending topic, the query parameter that can be used to search for the topic on Twitter-Search, the search URL and the volume of tweets for the last 24 hours, if available. (The trends get updated every 5 mins.)</p></li>\n<li><p>At query time <b><i>#BeratKandili, #GoodFriday</i></b> and <b><i>#WeLoveTheEarth</i></b> were trending WW.</p></li>\n<li><p><i>\"tweet_volume\"</i> tell us that <i>#WeLoveTheEarth</i> was the most popular among the three.</p></li>\n<li><p>Results are not sorted by <i>\"tweet_volume\"</i>. </p></li>\n<li><p>There are some trends which are unique to the US.</p></li>\n</ul>\n<hr>\n<p>It‚Äôs easy to skim through the two sets of trends and spot common trends, but let's not do \"manual\" work. We can use Python‚Äôs <strong>set</strong> data structure to find common trends ‚Äî we can iterate through the two trends objects, cast the lists of names to sets, and call the intersection method to get the common names between the two sets.</p>","metadata":{"dc":{"key":"17"},"run_control":{"frozen":true},"tags":["context"]},"id":"9c51b6b7-8de9-4d11-b026-0477d4246a2d","cell_type":"markdown"},{"source":"# Assuming you have the trend data in the format similar to a list of lists or a data frame\n\n# Extracting all the WW trend names from WW_trends\nworld_trends <- unique(WW_trends[[1]]$trends$name)\n\n# Extracting all the US trend names from US_trends\nus_trends <- unique(US_trends[[1]]$trends$name)\n\n# Let's get the intersection of the two sets of trends\ncommon_trends <- intersect(world_trends, us_trends)\n\n# Inspecting the data\ncat(\"World Trends:\\n\", world_trends, \"\\n\\n\")\ncat(\"US Trends:\\n\", us_trends, \"\\n\\n\")\ncat(length(common_trends), \"common trends:\", common_trends)\n","metadata":{"dc":{"key":"17"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"59c46710-0129-43c3-ba54-dc7a152cc458","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 4. Exploring the hot trend\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è From the intersection (last output) we can see that, out of the two sets of trends (each of size 50), we have 11 overlapping topics. In particular, there is one common trend that sounds very interesting: <i><b>#WeLoveTheEarth</b></i> ‚Äî so good to see that <em>Twitteratis</em> are unanimously talking about loving Mother Earth! üíö </p>\n<p><i><b>Note</b>: We could have had no overlap or a much higher overlap; when we did the query for getting the trends, people in the US could have been on fire obout topics only relevant to them.</i>\n<br>\n<img src=\"https://assets.datacamp.com/production/project_760/img/earth.jpg\" style=\"width: 500px\"></p>\n<div style=\"text-align: center;\"><i>Image Source:Official Music Video Cover: https://welovetheearth.org/video/</i></div>\n<hr>\n<p>We have found a hot-trend, #WeLoveTheEarth. Now let's see what story it is screaming to tell us! <br>\nIf we query Twitter's search API with this hashtag as query parameter, we get back actual tweets related to it. We have the response from the search API stored in the datasets folder as <i>'WeLoveTheEarth.json'</i>. So let's load this dataset and do a deep dive in this trend.</p>","metadata":{"dc":{"key":"24"},"run_control":{"frozen":true},"tags":["context"]},"id":"9035fd8b-6cac-4375-9294-261aa2e7968d","cell_type":"markdown"},{"source":"# Install and load necessary libraries\ninstall.packages(\"jsonlite\")\n\n# Load the JSON data\ntweets <- fromJSON(file = 'datasets/WeLoveTheEarth.json')\n\n# Inspecting some tweets\nprint(tweets[1:2,])\n","metadata":{"dc":{"key":"24"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"884be656-5e9b-46de-92e7-b7cd9c3b3332","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 5. Digging deeper\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è Printing the first two tweet items makes us realize that there‚Äôs a lot more to a tweet than what we normally think of as a tweet ‚Äî there is a lot more than just a short text!</p>\n<hr>\n<p>But hey, let's not get overwhemled by all the information in a tweet object! Let's focus on a few interesting fields and see if we can find any hidden insights there. </p>","metadata":{"dc":{"key":"31"},"run_control":{"frozen":true},"tags":["context"]},"id":"f92ae586-24e4-4893-83f3-87029a912427","cell_type":"markdown"},{"source":"# Assuming 'tweets' is a list containing tweet objects\n\n# Extracting the text of all the tweets from the tweet object\ntexts <- sapply(tweets, function(tweet) tweet$text)\n\n# Extracting screen names of users tweeting about #WeLoveTheEarth\nnames <- sapply(tweets, function(tweet) sapply(tweet$entities$user_mentions, function(user_mention) user_mention$screen_name))\n\n# Extracting all the hashtags being used when talking about this topic\nhashtags <- sapply(tweets, function(tweet) sapply(tweet$entities$hashtags, function(hashtag) hashtag$text))\n\n# Inspecting the first 10 results\nprint(jsonlite::toJSON(texts[1:10], pretty = TRUE))\nprint(jsonlite::toJSON(names[1:10], pretty = TRUE))\nprint(jsonlite::toJSON(hashtags[1:10], pretty = TRUE))\n","metadata":{"dc":{"key":"31"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"3c003f62-bb02-4679-8ee2-2b169a93e7d6","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 6. Frequency analysis\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è Just from the first few results of the last extraction, we can deduce that:</p>\n<ul>\n<li>We are talking about a song about loving the Earth.</li>\n<li>A lot of big artists are the forces behind this Twitter wave, especially Lil Dicky.</li>\n<li>Ed Sheeran was some cute koala in the song ‚Äî \"EdSheeranTheKoala\" hashtag! üê®</li>\n</ul>\n<hr>\n<p>Observing the first 10 items of the interesting fields gave us a sense of the data. We can now take a closer look by doing a simple, but very useful, exercise ‚Äî computing frequency distributions. Starting simple with frequencies is generally a good approach; it helps in getting ideas about how to proceed further.</p>","metadata":{"dc":{"key":"38"},"run_control":{"frozen":true},"tags":["context"]},"id":"b8ac0002-fb23-4206-a9bb-ed66447d38c2","cell_type":"markdown"},{"source":"# Assuming 'names' and 'hashtags' are vectors or columns in your dataset\n\n# Count occurrences/getting frequency distribution of all names and hashtags\nfor (item in c(names, hashtags)) {\n  freq_table <- table(item)\n  \n  # Inspecting the 10 most common items in the frequency table\n  print(head(sort(freq_table, decreasing = TRUE), 10))\n  cat(\"\\n\")\n}\n","metadata":{"dc":{"key":"38"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"d31a51a9-d7f0-4dab-94de-fd725c2d67e8","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 7. Activity around the trend\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è Based on the last frequency distributions we can further build-up on our deductions:</p>\n<ul>\n<li>We can more safely say that this was a music video about Earth (hashtag 'EarthMusicVideo') by Lil Dicky. </li>\n<li>DiCaprio is not a music artist, but he was involved as well <em>(Leo is an environmentalist so not a surprise to see his name pop up here)</em>. </li>\n<li>We can also say that the video was released on a Friday; very likely on April 19th. </li>\n</ul>\n<p><em>We have been able to extract so many insights. Quite powerful, isn't it?!</em></p>\n<hr>\n<p>Let's further analyze the data to find patterns in the activity around the tweets ‚Äî <b>did all retweets occur around a particular tweet? </b><br></p>\n<p>If a tweet has been retweeted, the <i>'retweeted_status'</i>  field gives many interesting details about the original tweet itself and its author. </p>\n<p>We can measure a tweet's popularity by analyzing the <b><i>retweet<em>count</em></i></b> and <b><i>favoritecount</i></b> fields. But let's also extract the number of followers of the tweeter  ‚Äî  we have a lot of celebs in the picture, so <b>can we tell if their advocating for #WeLoveTheEarth influenced a significant proportion of their followers?</b></p>\n<hr>\n<p><i><b>Note</b>: The retweet_count gives us the total number of times the original tweet was retweeted. It should be the same in both the original tweet and all the next retweets. Tinkering around with some sample tweets and the official documentaiton are the way to get your head around the mnay fields.</i></p>","metadata":{"dc":{"key":"45"},"run_control":{"frozen":true},"tags":["context"]},"id":"daf6f9cf-0cc4-424c-ba55-8b65df141585","cell_type":"markdown"},{"source":"# Assuming 'tweets' is a list of tweets in R\n# For illustration purposes, let's assume each tweet is a list or a data frame\n\n# Create a list to store extracted information from retweets\nretweets <- list()\n\n# Iterate through each tweet in the list\nfor (tweet in tweets) {\n  # Check if 'retweeted_status' is present in the tweet\n  if ('retweeted_status' %in% names(tweet)) {\n    # Extract the desired information and append to the retweets list\n    retweet_info <- c(\n      tweet$retweeted_status$field1,  # Replace 'field1', 'field2', etc. with actual field names\n      tweet$retweeted_status$field2,\n      tweet$retweeted_status$field3,\n      # Add more fields as needed\n    )\n    \n    retweets <- append(retweets, list(retweet_info))\n  }\n}\n\n# Print the extracted retweets information\nprint(retweets)\n","metadata":{"dc":{"key":"45"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"bdcd0fa9-9abf-42aa-b83e-e025d4015461","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 8. A table that speaks a 1000 words\n<p>Let's manipulate the data further and visualize it in a better and richer way ‚Äî <em>\"looks matter!\"</em></p>","metadata":{"dc":{"key":"52"},"run_control":{"frozen":true},"tags":["context"]},"id":"3500e91d-0ab2-439e-8173-01b46b85e5a0","cell_type":"markdown"},{"source":"# Install and load necessary libraries\ninstall.packages(\"formattable\")\nlibrary(formattable)\n\n# Assuming df is your data frame\n# Create a data frame (replace ... with your actual data)\ndf <- data.frame(\n  source_id = c(1, 2, 3),\n  publisher_name = c(\"source_name1\", \"source_name2\", \"source_name3\"),\n  author = c(\"author1\", \"author2\", \"author3\"),\n  title = c(\"title1\", \"title2\", \"title3\"),\n  description = c(\"description1\", \"description2\", \"description3\"),\n  url = c(\"url1\", \"url2\", \"url3\"),\n  url_to_image = c(\"image1\", \"image2\", \"image3\"),\n  published_at = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"),\n  content = c(\"content1\", \"content2\", \"content3\"),\n  top_article = c(TRUE, FALSE, TRUE),\n  engagement_reaction_count = c(10, 20, 30),\n  engagement_comment_count = c(5, 15, 25),\n  engagement_share_count = c(8, 18, 28),\n  engagement_comment_plugin_count = c(3, 13, 23)\n)\n\n# Create a formattable data frame with background gradient\nstyled_df <- as.datatable(df) %>%\n  formattable::color_tile(\n    color = \"lightblue\",\n    align = \"center\",\n    format = \"span\",\n    fontsize = 12\n  ) %>%\n  formattable::color_tile(\n    color = \"white\",\n    align = \"center\",\n    format = \"span\",\n    fontsize = 12\n  ) %>%\n  formattable::formatStyle(\n    names(df), \n    background = formattable::linearGradient(c(\"white\", \"lightblue\"))\n  )\n\n# Print the styled data frame\nprint(styled_df)\n","metadata":{"dc":{"key":"52"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"1b3eb922-2e3b-4b4f-9348-8222a1a2d27e","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 9. Analyzing used languages\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è Our table tells us that:</p>\n<ul>\n<li>Lil Dicky's followers reacted the most ‚Äî 42.4% of his followers liked his first tweet. </li>\n<li>Even if celebrities like Katy Perry and Ellen have a huuge Twitter following, their followers hardly reacted, e.g., only 0.0098% of Katy's followers liked her tweet. </li>\n<li>While Leo got the most likes and retweets in terms of counts, his first tweet was only liked by 2.19% of his followers. </li>\n</ul>\n<p>The large differences in reactions could be explained by the fact that this was Lil Dicky's music video. Leo still got more traction than Katy or Ellen because he played some major role in this initiative.</p>\n<hr>\n<p>Can we find some more interesting patterns in the data? From the text of the tweets, we could spot different languages, so let's create a frequency distribution for the languages.</p>","metadata":{"dc":{"key":"59"},"run_control":{"frozen":true},"tags":["context"]},"id":"c0c2ed6a-966b-4101-b1d4-8878fec87734","cell_type":"markdown"},{"source":"# Assuming 'tweets' is a vector containing the tweets\n\n# Load necessary libraries\ninstall.packages(\"tm\")\ninstall.packages(\"slam\")\ninstall.packages(\"ggplot2\")\n\nlibrary(tm)\nlibrary(slam)\nlibrary(ggplot2)\n\n# Create a function to detect language for each tweet\ndetect_language <- function(tweet) {\n  # You need to replace the language detection logic here\n  # This is just a placeholder\n  if (grepl(\"english\", tolower(tweet))) {\n    return(\"English\")\n  } else {\n    return(\"Unknown\")\n  }\n}\n\n# Apply language detection to each tweet\ntweets_languages <- sapply(tweets, detect_language)\n\n# Plotting the distribution of languages\nlanguage_counts <- table(tweets_languages)\nlanguage_df <- data.frame(Language = names(language_counts), Count = as.vector(language_counts))\n\nggplot(language_df, aes(x = Language, y = Count, fill = Language)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Distribution of Languages in Tweets\",\n       x = \"Language\",\n       y = \"Count\") +\n  theme_minimal()\n","metadata":{"dc":{"key":"59"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"e618b6e7-1dc0-40f9-88b8-1211636a2433","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 10. Final thoughts\n<p>üïµÔ∏è‚Äç‚ôÄÔ∏è The last histogram tells us that:</p>\n<ul>\n<li>Most of the tweets were in English.</li>\n<li>Polish, Italian and Spanish were the next runner-ups. </li>\n<li>There were a lot of tweets with a language alien to Twitter (lang = 'und'). </li>\n</ul>\n<p>Why is this sort of information useful? Because it can allow us to get an understanding of the \"category\" of people interested in this topic (clustering). We could also analyze the device type used by the Twitteratis, <code>tweet['source']</code>, to answer questions like, <strong>\"Does owning an Apple compared to Andorid influences people's propensity towards this trend?\"</strong>. I will leave that as a <strong>further exercise</strong> for you!</p>\n<p><img src=\"https://assets.datacamp.com/production/project_760/img/languages_world_map.png\" style=\"width: 500px\"></p>\n<hr>\n<p><span style=\"color:#41859e\">\nWhat an exciting journey it has been! We started almost clueless, and here we are.. rich in insights. </span></p>\n<p><span style=\"color:#41859e\">\nFrom location based comparisons to analyzing the activity around a tweet to finding patterns from languages and devices, we have covered a lot today ‚Äî let's give ourselves <b>a well-deserved pat on the back!</b> ‚úã\n</span>\n<br><br></p>\n<div style=\"text-align: center;color:#41859e\"><b><i>Magic Formula = Data + Python + Creativity + Curiosity</i></b></div>\n<p><img src=\"https://assets.datacamp.com/production/project_760/img/finish_line.jpg\" style=\"width: 500px\"></p>","metadata":{"dc":{"key":"66"},"run_control":{"frozen":true},"tags":["context"]},"id":"a1a9e4c7-0ddd-47aa-89f0-1964a40b4eb4","cell_type":"markdown"},{"source":"# Congratulations!\nprint(\"High Five!!!\")","metadata":{"dc":{"key":"66"},"tags":["sample_code"],"collapsed":true,"jupyter":{"outputs_hidden":true}},"id":"180ec1a1-fbec-43eb-930a-fa15b5a99bcf","cell_type":"code","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}